# Commands and workflow


<div id="tock" data-block_title="Summary"></div>
<div id="tocw"></div>

## `clerk build`

Build files or `clerk` targets listed in your [`clerk.toml` file](./6-1-clerk-toml.md).
This command is quite versatile, and can be used to build individual specific
files in any of the backends allowed by Catala (see help below), but the
Catala team advises to only use `clerk build` with targets properly defined
in `clerk.toml`.

The results of `clerk build` are available in the `_targets` directory by
default, if you have not specified another target directory in `clerk.toml`.
Each target generates a set of source code files in the target programming
languages, generated by the Catala compiler from the Catala source code
files. Please refer to the [compilation and deployment](./3-2-compilation-deployment.md)
walkthrough for more examples about how to use `clerk build` and the resulting
artifacts.

```admonish info collapsible=true title="clerk build --help"
<pre>
<!-- cmdrun clerk build --help=plain -->
</pre>
```

## `clerk test`

Discover, build and run tests.

### Running the tests

`clerk test` can be used without any arguments at the root directory of
a Catala project, with the following output:

```shell-session
$ clerk test
┏━━━━━━━━━━━━━━━━━━━━━━━━  ALL TESTS PASSED  ━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                                                                    ┃
┃             FAILED     PASSED      TOTAL                           ┃
┃   files          0         37         37                           ┃
┃   tests          0        261        261                           ┃
┃                                                                    ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The `tests` line of this report counts the number of failed and passed
tests. The `files` line displays the number of files where either all tests
pass, or there is at least one failing test.

You can print the details about the files containing passing and failing tests
with the `--verbose` flag. In addition, it is possible to run the
command with the flag `--xml` to obtain a JUnit-compatible report.

You can restrict the scope of which tests are executed by `clerk test` by providing another argument:
* `clerk test <file>` will run only the tests in `<file>`;
* `clerk test <folder>` will run only the tests inside files in `<folder>` (or its sub-directories);
* `clerk test <target>` will run only the tests related to the [build `<target>`](./6-1-clerk-toml.md).


~~~admonish info title="What does `clerk test` use for running tests?"
`clerk test` executes the tests with the Catala interpreter. If your deployment
uses a specific backend, say python, it is highly recommended that you also
include a run of `clerk test --backend=python` in your CI. With this option,
`clerk test` runs Python on the generated Python code by the Catala compiler.
 This way, you will be shielded from the eventuality that a bug in the backend
you use leads to a different outcome for the same Catala program. Trust does
not exclude checking throughouly!
~~~

### Declaring the tests

Catala supports two distinct flavors of tests, tailored for different purposes:

- **Scope tests** should be the main way to write tests that validate
  expected results on a given computation. This is the natural way to automate the
  `clerk run --scope=TestXxx` commands you use to run your tests manually.
- **Cram tests** provide a way to run custom catala commands and check their
  output on `stdout` and `stderr`. They are sometimes useful for more specific
  needs, like ensuring the correct error is triggered in a given situation.

The command `clerk test` can be run on any file or directory holding catala
files, will process both kinds of tests and print a report.

#### Scope tests

A **test scope** is a scope that is marked with the "test" [attribute](./5-7-1-attributes.md): simply
write `#[test]` just before its `declaration` keyword.

```catala
#[test]
declaration scope TestMoneyRounding:
  output result content money
```

There are two requirements for a test scope:
- The scope must be public (declared in a `` ```catala-metadata`` section) so
  that it can be run and checked in real conditions
- It must not require any input: only `internal`, `output` and `context`
  variables are allowed

The expected output of the test should be validated with `assertion` statements.
Without them, the only thing the test would validate is that the computation
doesn't trigger an error.

```catala
scope TestMoneyRounding:
  definition result equals $100 / 3
  assertion result = $33.33
```

As seen in [the tutorial](2-1-basic-blocks.html#testing-the-code), a test scope
almost always take the form of a call to the real scope you want to test,
providing it with specific inputs and an expected result:

```catala
#[test]
declaration scope Test_IncomeTaxComputation_1:
  output computation content IncomeTaxComputation

scope Test_IncomeTaxComputation_1:
  # Define the computation as IncomeTaxComputation applied to specific inputs
  definition computation equals
    output of IncomeTaxComputation with {
      -- individual:
        Individual {
          -- income: $20,000
          -- number_of_children: 0
        }
    }
  # Check that the result is as expected
  assertion computation.income_tax = $4,000
```

### Cram testing

Cram testing (or CLI testing) provides a means to validate the output of a given
Catala command, which may be useful in more specific integration scenarios. It
is inspired by the [Cram](https://bitheap.org/cram/) test system, in that a
single source file includes both the test command and its expected output.

For example, checking that an error happens when expected cannot be done with
test scopes, which must succeed. You may want to include tests that make use of
`catala proof`. Or you could want, for a simple test, to validate that the trace is
exactly as intended. For this, a `` ```catala-test-cli`` section should be
introduced in the source Catala file. The first line always starts with
`$ catala `, followed by the Catala command to run on the current file ; the
rest is the expected output from the command ; additionally, if the command
terminated with an error, the last line will show the error code.

~~~markdown
```catala-test-cli
$ catala interpret --scope=Test --trace
[LOG] ☛ Definition applied:
      ─➤ tutorial.catala_en:214.14-214.25:
          │
      214 │   definition computation equals
          │              ‾‾‾‾‾‾‾‾‾‾‾
      Test
[LOG] →  IncomeTaxComputation.direct
   ...
[LOG]   ≔  IncomeTaxComputation.direct.output: IncomeTaxComputation { -- income_tax: $4,000.00 }
[LOG] ←  IncomeTaxComputation.direct
[LOG] ≔  Test.computation: IncomeTaxComputation { -- income_tax: $4,000.00 }
┌─[RESULT]─ Test ─
│ computation = IncomeTaxComputation { -- income_tax: $4,000.00 }
└─
```
~~~

When running `clerk test`, the specified command is run on the file or directory (truncated
to that point). If the output exactly matches what is in the file, the tests
passes. Otherwise, it fails, and the precise differences are shown side-by-side.

Beware, cram tests this cannot be used to test backend-generated code; so `clerk
  test --backend=...` won't run cram tests.

~~~admonish example title="`test-scope`"
Note that for these `` ```catala-test-cli``, `$catala test-scope Test` is a shorthand for
```
$ catala interpret --scope=Test
```
~~~

~~~admonish tip title="Resetting the expected output of a cram test"
If a cram test fail, but due to a legitimate difference (for example, a line
number change in the example above), it is possible to run
`clerk test --reset` to automatically update the expected result. This will
immediately make the cram test pass, but versionning
systems and a standard code review will highlight the changes.

`clerk test --reset` can also be used to initialise a new test, from a
`` ```catala-test-cli`` section that only provides the command without expected
output.
~~~

```admonish info collapsible=true title="clerk test --help"
<pre>
<!-- cmdrun clerk test --help=plain -->
</pre>
```

## `clerk ci`

Scan the project and run all possible actions. This includes
the interpretation of all catala tests and CLI tests (equivalent to
running the [`clerk test`](#clerk-test) command), and also, the build of all clerk
targets (equivalent to running the [`clerk build`](#clerk-build) command) alongside the
execution of their tests against all their defined backend. This
command is useful for the execution of continuous integrations (CIs)
where all build and test actions are often meant to be executed.

```admonish info collapsible=true title="clerk ci --help"
<pre>
<!-- cmdrun clerk ci --help=plain -->
</pre>
```

## `clerk run`

Runs the Catala interpreter on the given files, after
building their dependencies. The scope to be executed can be specified
using the `-s` option.

As of writing, `clerk run` is restricted to scopes that do
not require inputs, so it is used to run test scopes.

### Example

`$ clerk run tests/tests_allocations_familiales.catala_fr -s Test1`

```admonish info collapsible=true title="clerk run --help"
<pre>
<!-- cmdrun clerk run --help=plain -->
</pre>
```

## `clerk clean`

Remove files and directories previously generated by `clerk`, notably
the `_build` and `_targets` directory. Useful to clean up the machine after
a CI job or to make sure you're not including stale, old files in your build
pipeline.

```admonish info collapsible=true title="clerk clean --help"
<pre>
<!-- cmdrun clerk clean --help=plain -->
</pre>
```
